import os
import pandas as pd
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import logging

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class CSVToElasticsearchImporter:
    def __init__(self):
        # Elasticsearch连接配置
        self.es = Elasticsearch(
            hosts=[{
                'host': '10.0.0.215',
                'port': 9200,
                'scheme': 'http'
            }]
        )
        self.index_name = "stockinfo"
        self.csv_directory = r"D:\03-code\pycharm\stock\flaskJuanwang_es\data\csv"

    def create_index_if_not_exists(self):
        """创建索引（如果不存在）"""
        if not self.es.indices.exists(index=self.index_name):
            # 创建索引映射
            mapping = {
                "mappings": {
                    "properties": {
                        "序号": {"type": "integer"},
                        "题干": {"type": "text"},
                        "选项 A": {"type": "text"},
                        "选项 B": {"type": "text"},
                        "选项 C": {"type": "text"},
                        "选项 D": {"type": "text"},
                        "选项 E": {"type": "text"},
                        "选项 F": {"type": "text"},
                        "选项 G": {"type": "text"},
                        "选项 H": {"type": "text"},
                        "解析": {"type": "text"},
                        "分数": {"type": "keyword"},
                        "答案": {"type": "text"},
                        "标签": {"type": "keyword"}
                    }
                }
            }

            try:
                self.es.indices.create(index=self.index_name, body=mapping)
                logger.info(f"索引 {self.index_name} 创建成功")
            except Exception as e:
                logger.error(f"创建索引失败: {e}")
                return False
        else:
            logger.info(f"索引 {self.index_name} 已存在")
        return True

    def read_csv_files(self):
        """读取目录中的所有CSV文件"""
        csv_files = []
        if not os.path.exists(self.csv_directory):
            logger.error(f"目录 {self.csv_directory} 不存在")
            return csv_files

        for file in os.listdir(self.csv_directory):
            if file.endswith('.csv'):
                csv_files.append(os.path.join(self.csv_directory, file))

        logger.info(f"找到 {len(csv_files)} 个CSV文件")
        return csv_files

    def process_csv_to_bulk(self, file_path):
        """将CSV文件处理成bulk操作格式"""
        actions = []

        try:
            # 使用pandas读取CSV文件
            df = pd.read_csv(file_path, encoding='utf-8')

            # 处理每一行数据
            for index, row in df.iterrows():
                # 将NaN值替换为None，确保数据类型正确
                doc = {}
                for col in df.columns:
                    value = row[col]
                    # 处理NaN和空值
                    if pd.isna(value):
                        doc[col] = None
                    else:
                        # 尝试转换数字类型
                        if col == "序号":
                            try:
                                doc[col] = int(value)
                            except:
                                doc[col] = value
                        else:
                            doc[col] = str(value)

                action = {
                    "_index": self.index_name,
                    "_source": doc
                }
                actions.append(action)

        except Exception as e:
            logger.error(f"处理文件 {file_path} 时出错: {e}")

        return actions

    def import_all_csv(self):
        """导入所有CSV文件到Elasticsearch"""
        # 创建索引
        if not self.create_index_if_not_exists():
            logger.error("无法创建索引，程序退出")
            return False

        # 获取所有CSV文件
        csv_files = self.read_csv_files()

        if not csv_files:
            logger.warning("没有找到CSV文件")
            return False

        total_imported = 0
        total_failed = 0

        # 处理每个CSV文件
        for file_path in csv_files:
            logger.info(f"正在处理文件: {os.path.basename(file_path)}")

            # 转换为bulk操作格式
            actions = self.process_csv_to_bulk(file_path)

            if not actions:
                logger.warning(f"文件 {os.path.basename(file_path)} 没有有效数据")
                continue

            try:
                # 批量导入数据
                success, failed = bulk(self.es, actions, chunk_size=1000, request_timeout=60)
                total_imported += success
                total_failed += len(failed) if failed else 0
                logger.info(f"文件 {os.path.basename(file_path)} 导入完成: 成功{success}条记录")

                if failed:
                    logger.warning(f"文件 {os.path.basename(file_path)} 导入失败: {len(failed)} 条记录")

            except Exception as e:
                logger.error(f"导入文件 {os.path.basename(file_path)} 时出错: {e}")

        logger.info(f"导入完成 - 总共成功导入 {total_imported} 条记录，失败 {total_failed} 条记录")
        return True


def main():
    importer = CSVToElasticsearchImporter()
    success = importer.import_all_csv()
    if success:
        logger.info("所有CSV文件导入完成")
    else:
        logger.error("导入过程中出现错误")


if __name__ == "__main__":
    main()
